---
title: Large language models
date: 2025-02-01
tags:
  - spark
  - knowledge
---
> [!todo]- Ideas to add on
> 
> * What are LLMs?
> * What's the history behind them?
> * How are they evolving now?
# Interesting terms

* **Catastrophic inteference/forgetting** — tendency for knowledge of the previously learned task(s) to be abruptly lost as information relevant to the current task is incorporated[^1]
* **Knowledge distillation** — technique that transfers the learnings of a large pre-trained model ("teacher model") to a smaller one ("student model")[^2]

[^1]: As defined in [this research paper](https://www.pnas.org/doi/10.1073/pnas.1611835114)
[^2]: As defined in [this IBM blog post](https://www.ibm.com/think/topics/knowledge-distillation)